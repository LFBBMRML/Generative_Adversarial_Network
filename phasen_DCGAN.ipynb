{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "phasen_DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOvMorv64/nZvhOB48S/wP1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LFBBMRML/Generative_Adversarial_Network/blob/main/phasen_DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chvBWRGLoToz"
      },
      "source": [
        "%tensorflow_version 2.x\r\n",
        "import tensorflow as tf\r\n",
        "tf.__version__\r\n",
        "%load_ext tensorboard\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras.utils import plot_model\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "from numpy import expand_dims\r\n",
        "from numpy import zeros\r\n",
        "from numpy import ones\r\n",
        "from numpy import vstack\r\n",
        "from numpy.random import randn\r\n",
        "from numpy.random import randint\r\n",
        "from keras import optimizers\r\n",
        "\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense\r\n",
        "from keras.layers import Flatten\r\n",
        "from keras.layers import Conv2D\r\n",
        "from keras.layers import Conv2DTranspose\r\n",
        "from keras.layers import LeakyReLU\r\n",
        "from keras.layers import Dropout\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import datetime\r\n",
        "import time as zeit\r\n",
        "from datetime import time\r\n",
        "from tensorboard import notebook\r\n",
        "from keras import layers\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ4TYyFfocHm"
      },
      "source": [
        "\r\n",
        "def define_discriminator():\r\n",
        "  D = Sequential()\r\n",
        "\r\n",
        "  D.add(Conv2D(64, (3, 3), padding = 'same', input_shape=(32, 32, 3)))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Conv2D(128, (3, 3), strides=(2,2), padding='same'))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Conv2D(128, (3, 3), strides=(2,2), padding = 'same'))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Conv2D(256, (3, 3), strides=(2,2), padding = 'same'))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Flatten())\r\n",
        "  D.add(Dropout(0.4))\r\n",
        "  D.add(Dense(1, activation = 'sigmoid'))\r\n",
        "\r\n",
        "  opt = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\r\n",
        "  D.compile(loss='binary_crossentropy', optimizer=opt, metrics = ['accuracy'])\r\n",
        "\r\n",
        "  D.summary()\r\n",
        "\r\n",
        "  return D\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuDLzu-rVLEe"
      },
      "source": [
        "\"\"\"\r\n",
        "def p_define_discriminator():     #STURKTUR_1\r\n",
        "  D = Sequential()\r\n",
        "\r\n",
        "  D.add(Conv2D(64, (5, 5), padding = 'same', input_shape=(32, 32, 3)))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Conv2D(128, (5, 5), strides=(8,8), padding = 'same'))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Flatten())\r\n",
        "  D.add(Dropout(0.4))\r\n",
        "  D.add(Dense(1, activation = 'sigmoid'))\r\n",
        "\r\n",
        "  opt = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\r\n",
        "  D.compile(loss='binary_crossentropy', optimizer=opt, metrics = ['accuracy'])\r\n",
        "\r\n",
        "  D.summary()\r\n",
        "  \r\n",
        "\r\n",
        "  return D\r\n",
        "\r\n",
        "#p_d_model = p_define_discriminator()\r\n",
        "#plot_model(p_d_model, to_file='pDis-Test.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fhWtGIFX78k"
      },
      "source": [
        "\r\n",
        "def p_define_discriminator():   #STURKTUR_2\r\n",
        "  D = Sequential()\r\n",
        "\r\n",
        "  D.add(Conv2D(64, (3, 3), padding = 'same', input_shape=(32, 32, 3)))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Conv2D(128, (2, 2), strides=(1,1), padding='same'))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Conv2D(128, (2, 2), strides=(1,1), padding = 'same'))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Conv2D(256, (2, 2), strides=(2,2), padding = 'same'))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Conv2D(256, (2, 2), strides=(2,2), padding = 'same'))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Conv2D(256, (2, 2), strides=(2,2), padding = 'same'))\r\n",
        "  D.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  D.add(Flatten())\r\n",
        "  D.add(Dropout(0.4))\r\n",
        "  D.add(Dense(1, activation = 'sigmoid'))\r\n",
        "\r\n",
        "  opt = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\r\n",
        "  D.compile(loss='binary_crossentropy', optimizer=opt, metrics = ['accuracy'])\r\n",
        "\r\n",
        "  D.summary()\r\n",
        "\r\n",
        "  return D\r\n",
        "\r\n",
        "#p_d_model = p_define_discriminator()\r\n",
        "#plot_model(p_d_model, to_file='pDis-Test.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dahUTkeogJv"
      },
      "source": [
        "\r\n",
        "def define_generator(latent_dim):\r\n",
        "  G = Sequential()\r\n",
        "\r\n",
        "  G.add(Dense(256 * 4 * 4, input_dim=latent_dim))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "  G.add(layers.Reshape((4, 4, 256)))\r\n",
        "\r\n",
        "  G.add(Conv2DTranspose(128, (4, 4), strides=(2,2), padding='same'))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  G.add(Conv2DTranspose(128, (4, 4), strides=(2,2), padding='same'))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  G.add(Conv2DTranspose(128, (4, 4), strides=(2, 2), padding='same'))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  G.add(Conv2D(3, (3 ,3), activation='tanh', padding='same'))\r\n",
        "  \r\n",
        "  G.summary()\r\n",
        "\r\n",
        "  return G\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huv6sHTrqUNW"
      },
      "source": [
        "\"\"\"\r\n",
        "def p_define_generator(latent_dim):     #STURKTUR_1\r\n",
        "  G = Sequential()\r\n",
        "\r\n",
        "  G.add(Dense(128 * 4 * 4, input_dim=latent_dim))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "  G.add(layers.Reshape((4, 4, 128)))\r\n",
        "\r\n",
        "  G.add(Conv2DTranspose(64, (5, 5), strides=(8, 8), padding='same'))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  G.add(Conv2D(3, (3 ,3), activation='tanh', padding='same'))\r\n",
        "  \r\n",
        "  G.summary()\r\n",
        "\r\n",
        "  return G\r\n",
        "\r\n",
        "#p_g_model = p_define_generator(100)\r\n",
        "#plot_model(p_g_model, to_file='p_Gen-Test.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SmNrDfEXpNV"
      },
      "source": [
        "\r\n",
        "def p_define_generator(latent_dim):        #STURKTUR_2\r\n",
        "  G = Sequential()\r\n",
        "\r\n",
        "  G.add(Dense(256 * 4 * 4, input_dim=latent_dim))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "  G.add(layers.Reshape((4, 4, 256)))\r\n",
        "\r\n",
        "  G.add(Conv2DTranspose(256, (2, 2), strides=(2,2), padding='same'))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  G.add(Conv2DTranspose(256, (2, 2), strides=(2,2), padding='same'))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  G.add(Conv2DTranspose(128, (2, 2), strides=(2,2), padding='same'))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  G.add(Conv2DTranspose(128, (2, 2), strides=(1,1), padding='same'))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  G.add(Conv2DTranspose(64, (2, 2), strides=(1, 1), padding='same'))\r\n",
        "  G.add(LeakyReLU(alpha=0.2))\r\n",
        "\r\n",
        "  G.add(Conv2D(3, (3 ,3), activation='tanh', padding='same'))\r\n",
        "  \r\n",
        "  G.summary()\r\n",
        "\r\n",
        "  return G\r\n",
        "\r\n",
        "#p_g_model = p_define_generator(100)\r\n",
        "#plot_model(p_g_model, to_file='p_Gen-Test.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COavmTOoohaW"
      },
      "source": [
        "def define_gan(g_model, d_model):\r\n",
        "  d_model.trainable = False\r\n",
        "\r\n",
        "  model = Sequential()\r\n",
        "  model.add(g_model)\r\n",
        "  model.add(d_model)\r\n",
        "\r\n",
        "  opt = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\r\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer=opt)\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h40mAw6Wwo6X"
      },
      "source": [
        "def define_phase_A(p_g_model, d_model):\r\n",
        "  d_model.trainable = False\r\n",
        "\r\n",
        "  model = Sequential()\r\n",
        "  model.add(p_g_model)\r\n",
        "  model.add(d_model)\r\n",
        "\r\n",
        "  opt = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\r\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer=opt)\r\n",
        "\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQcEaPQIws_9"
      },
      "source": [
        "def define_phase_B(g_model, p_d_model):\r\n",
        "  p_d_model.trainable = False\r\n",
        "\r\n",
        "  model = Sequential()\r\n",
        "  model.add(g_model)\r\n",
        "  model.add(p_d_model)\r\n",
        "\r\n",
        "  opt = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\r\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer=opt)\r\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5Ho9zx7ohX-"
      },
      "source": [
        "def load_real_samples():\r\n",
        "  (train, _), (_,_) = tf.keras.datasets.cifar10.load_data()\r\n",
        "  images = train.astype('float32')\r\n",
        "  images = (images - 127.5) / 127.5\r\n",
        "  return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVf2hSQ1ohVO"
      },
      "source": [
        "def generate_real_samples(dataset, n_samples):\r\n",
        "  ix = randint(0, dataset.shape[0], n_samples)\r\n",
        "  X = dataset[ix]\r\n",
        "  y = ones((n_samples, 1))\r\n",
        "\r\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gv9IB6AohSe"
      },
      "source": [
        "def generate_latent_points(latent_dim, n_samples):\r\n",
        "  x_input = randn(latent_dim * n_samples)     #Return a sample (or samples) from the “standard normal” distribution\r\n",
        "  x_input = x_input.reshape(n_samples, latent_dim)\r\n",
        "  return x_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fUnhwvMohP6"
      },
      "source": [
        "def generate_fake_samples(g_model, latent_dim, n_samples):\r\n",
        "  x_input = generate_latent_points(latent_dim, n_samples)\r\n",
        "  X = g_model.predict(x_input)\r\n",
        "  y = zeros((n_samples, 1))\r\n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqWg35f8ohMt"
      },
      "source": [
        "def save_plot(examples, epoch, n=6):\r\n",
        "  examples = (examples + 1) / 2.0\r\n",
        "\r\n",
        "  for i in range(n*n):\r\n",
        "    plt.subplot(n, n, i+1)\r\n",
        "    plt.axis('off')\r\n",
        "    plt.imshow(examples[i])\r\n",
        "  #filename= 'Struktur1_Epoche%03d.png' % (epoch+1)\r\n",
        "  filename= 'Struktur2_Epoche%03d.png' % (epoch+1)\r\n",
        "  plt.savefig(filename)\r\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z98_A-siosgy"
      },
      "source": [
        "latent_dim = 100\r\n",
        "d_model = define_discriminator()\r\n",
        "g_model = define_generator(latent_dim)\r\n",
        "gan_model = define_gan(g_model, d_model)\r\n",
        "p_d_model = p_define_discriminator()\r\n",
        "p_g_model = p_define_generator(latent_dim)\r\n",
        "phase_A_model = define_phase_A(p_g_model, d_model)\r\n",
        "phase_B_model = define_phase_B(g_model, p_d_model)\r\n",
        "dataset = load_real_samples()\r\n",
        "EPOCHEN = 1\r\n",
        "anz_epochen = 100\r\n",
        "epoche_p = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M316QJwvohFd"
      },
      "source": [
        "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=150):\r\n",
        "  X_real, y_real = generate_real_samples(dataset, n_samples)\r\n",
        "  dis_loss_on_true, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\r\n",
        "\r\n",
        "  X_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)\r\n",
        "  dis_loss_on_fake, acc_fake = d_model.evaluate(X_fake, y_fake, verbose=0)\r\n",
        "\r\n",
        "  print('>Zu wie viel Prozent der Diskriminator echte Bilder: %.0f%% und Fake Bilder:%.0f%% richtig zuordnet' % (acc_real*100, acc_fake*100))\r\n",
        "  \r\n",
        "  save_plot(X_fake, epoch)  \r\n",
        "  filename = 'generierte_Ausgabe_%03d.h5' % (epoch+1)\r\n",
        "  g_model.save(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0ArCPMoYbfN"
      },
      "source": [
        "\"\"\" \r\n",
        "#TENSORBOARD_TEIL1\r\n",
        "def write_log(names, logs, steps):\r\n",
        "  writer = tf.summary.create_file_writer('logs')\r\n",
        "  with writer.as_default():\r\n",
        "    tf.summary.scalar(name=names, data=logs, step=steps)\r\n",
        "    \r\n",
        "    #tf.summary.histogram(name=names, data=logs, step=batch_no)\r\n",
        "    writer.flush()\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5XfsDxnog7f"
      },
      "source": [
        "def train(g_model, d_model, gan_model, p_d_model, p_g_model, phase_A_model, phase_B_model, dataset, latent_dim, n_epochs=EPOCHEN, anz_epochen=anz_epochen, epoche_p=epoche_p, n_batch=64):\r\n",
        "  bat_per_epo = int(dataset.shape[0] / n_batch)\r\n",
        "  half_batch = int(n_batch/2)\r\n",
        "  ges_time = []\r\n",
        "  for i in range(anz_epochen):\r\n",
        "    start = zeit.time()\r\n",
        "    for phase_a in range(epoche_p):\r\n",
        "      for j in range(bat_per_epo):\r\n",
        "        X_real_a, y_real_a = generate_real_samples(dataset, half_batch) # nimmt ein Bild des Datensatzes und 1 Array\r\n",
        "        d_loss1_a, _ = d_model.train_on_batch(X_real_a, y_real_a)\r\n",
        "\r\n",
        "        X_fake_a, y_fake_a = generate_fake_samples(p_g_model, latent_dim, half_batch) # generiertes Bild und 0 Array\r\n",
        "        d_loss2_a, _ = d_model.train_on_batch(X_fake_a, y_fake_a)\r\n",
        "\r\n",
        "        d_loss_a = 0.5 * (d_loss1_a + d_loss2_a)\r\n",
        "\r\n",
        "        X_gan_a = generate_latent_points(latent_dim, n_batch)\r\n",
        "        y_gan_a = ones((n_batch, 1))\r\n",
        "        g_loss_a = phase_A_model.train_on_batch(X_gan_a, y_gan_a)\r\n",
        "\r\n",
        "        #print(\"AUSFUEHRUNG_PhaseA\")\r\n",
        "        print('Phase_A >%d, %d, %d/%d, d1=%.3f, d2=%.3f, d=%.3f, g=%.3f' % (i+1, phase_a+1, j+1, bat_per_epo, d_loss1_a, d_loss2_a, d_loss_a, g_loss_a))\r\n",
        "        \r\n",
        "        #write_log('phase_A_loss', g_loss_a, i+1) #TENSORBOAR_TEIL2\r\n",
        "        \r\n",
        "\r\n",
        "    for phase_b in range(epoche_p):\r\n",
        "      for j in range(bat_per_epo):\r\n",
        "        X_real_b, y_real_b = generate_real_samples(dataset, half_batch) # nimmt ein Bild des Datensatzes und 1 Array\r\n",
        "        d_loss1_b, _ = p_d_model.train_on_batch(X_real_b, y_real_b)\r\n",
        "  \r\n",
        "        X_fake_b, y_fake_b = generate_fake_samples(g_model, latent_dim, half_batch) # generiertes Bild und 0 Array\r\n",
        "        d_loss2_b, _ = p_d_model.train_on_batch(X_fake_b, y_fake_b)\r\n",
        "\r\n",
        "        d_loss_b = 0.5 * (d_loss1_b + d_loss2_b)\r\n",
        "\r\n",
        "        X_gan_b = generate_latent_points(latent_dim, n_batch)\r\n",
        "        y_gan_b = ones((n_batch, 1))\r\n",
        "        g_loss_b = phase_B_model.train_on_batch(X_gan_b, y_gan_b)\r\n",
        "        #print(\"AUSFUEHRUNG_PhaseB\")\r\n",
        "        print('Phase_B >%d, %d, %d/%d, d1=%.3f, d2=%.3f, d=%.3f, g=%.3f' % (i+1, phase_b+1, j+1, bat_per_epo, d_loss1_b, d_loss2_b, d_loss_b, g_loss_b))\r\n",
        "  \r\n",
        "        #write_log('phase_B_loss', g_loss_b, i+1) #TENSORBOAR_TEIL2\r\n",
        "        \r\n",
        "\r\n",
        "    for gan in range(n_epochs):\r\n",
        "      for j in range(bat_per_epo):\r\n",
        "        X_real, y_real = generate_real_samples(dataset, half_batch) # nimmt ein Bild des Datensatzes und 1 Array\r\n",
        "        d_loss1, _ = d_model.train_on_batch(X_real, y_real)\r\n",
        "  \r\n",
        "        X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch) # generiertes Bild und 0 Array\r\n",
        "        d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)\r\n",
        "\r\n",
        "        d_loss = 0.5 * (d_loss1 + d_loss2)\r\n",
        "\r\n",
        "        X_gan = generate_latent_points(latent_dim, n_batch)\r\n",
        "        y_gan = ones((n_batch, 1))\r\n",
        "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\r\n",
        "        #print(\"AUSFUEHRUNG_GAN\")\r\n",
        "        print('GAN >%d, %d, %d/%d, d1=%.3f, d2=%.3f, d=%.3f, g=%.3f' % (i+1, gan+1, j+1, bat_per_epo, d_loss1, d_loss2, d_loss, g_loss))\r\n",
        "  \r\n",
        "        #write_log('GAN_loss', g_loss, i+1) #TENSORBOAR_TEIL2\r\n",
        "        \r\n",
        "\r\n",
        "    \r\n",
        "    #plot_model(d_model, to_file='Diskriminator.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n",
        "    #plot_model(p_d_model, to_file='p_Dis.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n",
        "    #plot_model(g_model, to_file='Generator.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n",
        "    #plot_model(p_g_model, to_file='p_Gen.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n",
        "    #plot_model(phase_A_model, to_file='pPhase_A.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n",
        "    #plot_model(phase_B_model, to_file='pPhase_B.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n",
        "    #plot_model(gan_model, to_file='pGAN.png', show_shapes=True, show_layer_names=True, rankdir='TB')\r\n",
        "    \r\n",
        "\r\n",
        "    ges_time.append(zeit.time()-start)\r\n",
        "    minuten = int((sum(ges_time))/60)\r\n",
        "    sec = sum(ges_time)%60\r\n",
        "\r\n",
        "    if (i+1) % 1 == 0:\r\n",
        "      summarize_performance(i, g_model, d_model, dataset, latent_dim)\r\n",
        "    print(\"Die gesamte Dauer beträgt: \" + str(minuten) + \" Minuten und \" + str(sec) + \" Sekunde, für \" + str(i+1) + \" Epochen \")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmirxdAIosdd"
      },
      "source": [
        "train(g_model, d_model, gan_model, p_d_model, p_g_model, phase_A_model, phase_B_model, dataset, latent_dim)\r\n",
        "#%tensorboard --logdir logs\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}